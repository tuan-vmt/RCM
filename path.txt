http://10.255.60.6/vu/research-recommendation-system/tree/develop/GEM-2022/Thang%2012/RCM-main
http://10.255.60.6/vu/research-recommendation-system/tree/develop/GEM-2022/Thang%2012/Tai%20lieu
http://10.255.60.6/vu/research-recommendation-system/tree/develop/GEM-2022/Thang%2012/bbnt
http://10.255.60.6/thangnq26/vtg_os_etl/tree/gem-2022/GEM_2022/Thang%2012/%20Model%20RM%20h%E1%BB%99%20gia%20%C4%91%C3%ACnh
http://10.255.60.6/vu/research-recommendation-system/blob/develop/GEM-2022/Thang%2012/RCM-Adtech

----LIST DAY----
import os
from datetime import datetime, timezone, timedelta

a =datetime.now()
print(a.strftime('%Y%m%d'))
list_file = sorted(os.listdir("f_tv360_log_film_daily/"))
print(list_file)

path_list_file_log_film = []
today = "20220701"
numbers_of_day = 30
for i in range(1, numbers_of_day + 1):
    given_date = datetime.strptime(today, '%Y%m%d')
    past_date = given_date - timedelta(days=i)
    past_date_str = past_date.strftime('%Y%m%d')
    file_path = "/home/gem/Pictures/RCM/f_tv360_log_film_daily/" + "log_film_daily_from_" + past_date_str + ".csv"
    if os.path.exists(file_path):
        print("HIHI")
        exit()
    path_list_file_log_film.append("log_film_daily_from_" + past_date_str + ".csv")
print(path_list_file_log_film)


from airflow.models import DagBag
dag_ids = DagBag(include_examples=False).dag_ids
for id in dag_ids:
   print(id)

# tai lieu adtech
https://medium0.com/@aliozan_memetoglu/4-collaborative-filtering-and-knn-f997f8993256?source=user_profile---------3----------------------------

import numpy as np
import pandas as pd
import jaydebeapi
from datetime import datetime, timedelta
import os
from datetime import date

#Connect DB
import jaydebeapi
conn = jaydebeapi.connect("org.apache.hive.jdbc.HiveDriver",
                          "jdbc:hive2://10.60.170.14:8883/default",
                          {'user': "dgx_v", 'password': "dslkAekj#@255"},
                           jars="/rapids/notebooks/hive.jar")
curs = conn.cursor()

curs.execute("show columns from d_tv360_film_series_info")

def get_log_film_daily_from_day(day, num_pre_day):
    f = open("run.txt", "a")
    f.write("Run Now!")
    f.close()
    given_date = datetime.strptime(day, '%Y%m%d')
    past_date = given_date - timedelta(days=num_pre_day)
    past_date_str = past_date.strftime('%Y%m%d')
    
    curs.execute("show columns from f_tv360_log_film_daily")
    column_log_film_daily = curs.fetchall()
    columns_log_film_daily = []
    for ele in column_log_film_daily:
        columns_log_film_daily.append(ele[0])
    
    curs.execute("select * from f_tv360_log_film_daily where partition == {}".format(past_date_str))
    result_log_film_daily = curs.fetchall()
    log_film_daily = pd.DataFrame(result_log_film_daily, columns=columns_log_film_daily)
    print(past_date_str)
    output_dir = "/tf/os/RCM-main/data/f_tv360_log_film_daily/"
    if not os.path.exists(output_dir):
            os.makedirs(output_dir)
    log_film_daily.to_csv('{}/log_film_daily_from_{}.csv'.format(output_dir, past_date_str), index=False)
    
    return past_date_str

def download_log_film():
    today = "20220701"
    numbers_of_day = 30
    for i in range(numbers_of_day):
        past_date_str = get_log_film_daily_from_day(today, i)

# today = "20220701"
# download_log_film(today, 30)



import numpy as np
import pandas as pd
import jaydebeapi
import csv
from datetime import date, datetime, timedelta
import os
import time 
import random
import dask.dataframe as dd
import argparse
import torch
from torch.utils.data import TensorDataset, DataLoader, IterableDataset, Dataset
import os
import torch.nn.functional as F
import torch 
import torch.nn as nn   
from tqdm import tqdm
import math
from transformers import AutoModel,AutoTokenizer
from pyvi import ViTokenizer
import pickle
from config_path import opt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

conn = jaydebeapi.connect("org.apache.hive.jdbc.HiveDriver",
                          "jdbc:hive2://10.60.170.14:8883/adp_new_db",
                          {'user': "dgx_v", 'password': "dslkAekj#@255"},
                           jars="/rapids/notebooks/hive.jar")
curs = conn.cursor()

def drop_partition(table_name, partition):
    curs.execute(f"ALTER TABLE adp_new_db.{table_name} SET TBLPROPERTIES('EXTERNAL'='False' )")
    curs.execute(f"ALTER TABLE adp_new_db.{table_name} DROP IF EXISTS PARTITION(partition={partition})")
    curs.execute(f"ALTER TABLE adp_new_db.{table_name} SET TBLPROPERTIES('EXTERNAL'='True' )")
    
def create_d_table(table_name):
    curs.execute(f"""
        CREATE TABLE IF NOT EXISTS adp_new_db.{table_name}

    (

           video_id string,
            video_embedding array<float>

    )
    STORED AS PARQUET
    LOCATION '/work_zone/upsell_vas/vas/production/tmp/{table_name}'
    TBLPROPERTIES ("parquet.compression" = "SNAPPY")
    """)
    
def create_f_table(table_name):
    curs.execute(f"""
        CREATE TABLE IF NOT EXISTS adp_new_db.{table_name}

    (

           video_id string,
            video_embedding array<float>

    )
    partitioned by (partition string )
    STORED AS PARQUET
    LOCATION '/work_zone/upsell_vas/vas/production/tmp/{table_name}'
    TBLPROPERTIES ("parquet.compression" = "SNAPPY")
    """)
    
def drop_table(table_name):
    curs.execute(f'''ALTER TABLE adp_new_db.{table_name} SET TBLPROPERTIES('AUTO.PURGE'='TRUE', 'EXTERNAL'='FALSE')''')
    curs.execute(f'''DROP TABLE adp_new_db.{table_name}''')
    
def write_data_hdfs(f_table_name, input_path, d_table_name, YYYYMMDD_Dsub1):
    with open(input_path, 'rb') as f:
        data = pickle.load(f)
            
    item_id = list(data.keys())
    item_id = [int(id) for id in item_id]
    item_embedding = list(data.values())
    print(item_id)
    print(item_embedding)
    if len(item_id) == 0:
        print(f"Partition {YYYYMMDD_Dsub1} dont have data")
        return 0
    create_f_table(f_table_name)
    
    drop_partition(f_table_name, YYYYMMDD_Dsub1)
    
    ## Tao cau lenh query
    insert_val = f"insert into adp_new_db.{d_table_name} values"
    curs.execute("SET mapred.reduce.tasks=1")
    for x in range(len(item_id)):
        if (x % 5000 == 0) | (x == len(item_id)-1): 
            print(f"Insert {x+1} rows to partition {YYYYMMDD_Dsub1}")
            insert_val += f"({item_id[x]}, array{tuple(item_embedding[x])})"
            ## Tạo bảng d
            create_d_table(d_table_name)

            ## Insert data vào bảng d
            curs.execute(f"{insert_val}")

            ## Inser data từ bảng d vào bảng f
            curs.execute(f"INSERT into TABLE adp_new_db.{f_table_name} PARTITION(partition={YYYYMMDD_Dsub1}) SELECT * FROM adp_new_db.{d_table_name} cluster by video_id")

            ## Drop bang d
            drop_table(d_table_name)

            insert_val = f"insert into adp_new_db.{d_table_name} values"
        else:
            insert_val += f"({item_id[x]}, array{tuple(item_embedding[x])}), "
            
if __name__ == '__main__':
    
#     YYYYMMDD_Dsub1 = (datetime.strptime(args.partition, '%Y%m%d') - timedelta(days=1)).strftime('%Y%m%d')
    YYYYMMDD_Dsub1 = "20221226"
    
    print(f"Query on parition = {YYYYMMDD_Dsub1}")
    
    table_name = "f_tv360_dev_film_os_gem"
    f_table_name = "f_tv360_film_embedding_os_gem"
    d_table_name = "d_tv360_film_embedding_of_gem"
    input_path = "/tf/os/RCM-main/TV360/RCM/save_inference/inference_users_film_" + f"{YYYYMMDD_Dsub1}" + ".pickle"
    #Push to hdfs
    write_data_hdfs(f_table_name, input_path, d_table_name, YYYYMMDD_Dsub1)            
